# Sales Conversation Simulator

This project simulates sales conversations between an AI-powered seller agent and a persona-based buyer agent using LangGraph, Google Gemini models, and the eBay API. The goal is to analyze sales strategies and agent interactions in a controlled environment.

## Features

*   **LangGraph Simulation:** Orchestrates the multi-turn conversation flow between agents.
*   **Persona-Based Buyers:** Buyer agents adopt personas defined in text files (derived from NMF analysis), influencing their behavior and interests.
*   **Gemini-Powered Agents:** Uses Google Gemini models (via Langchain) for both seller and buyer agent reasoning and responses.
*   **eBay Tool Integration:** Seller agent can use tools to search eBay for products and retrieve item details via the eBay Browse API.
*   **LLM-Based Sale Analysis:** Analyzes conversation history using an LLM to detect if a sale was completed and extract relevant details.
*   **Product Ranking:** Calculates the semantic similarity rank of a sold item within its relevant product category index (CSV file).
*   **Database Logging:** Logs all simulation messages and completed sale records to a PostgreSQL database.
*   **Reporting:** Generates a Markdown report summarizing results across multiple simulation runs, including extracted "sales wisdom" generated by an LLM.

## Project Structure

```
├── core/                 # Core simulation logic (LangGraph, agents, analysis, state)
├── data/
│   └── personas/         # Persona prompts (.txt) and product indices (.csv)
├── models/               # Pre-trained models (e.g., nmf_model.pkl) - *potentially*
├── tests/                # Unit and integration tests
├── tools/                # API interaction wrappers (e.g., ebay_api.py)
├── utils/                # Utility functions (DB connection, persona loading)
├── .env.example          # Example environment variables file
├── .gitignore
├── generate_report.py    # Script to run multiple simulations and generate report
├── requirements.txt      # Python dependencies
├── run_simulation.py     # Script to run a single simulation
├── setup_sales_sim_db.sql # SQL script to set up database schema
├── simulation_report.md  # Example output report (can be gitignored if preferred)
└── README.md             # This file
```

## Setup Instructions

1.  **Navigate to Project Directory:**
    *   Make sure your terminal is in the `Phase_2/eBay_Simulation/` directory before proceeding.
    ```bash
    cd Phase_2/eBay_Simulation/
    ```

2.  **Create and Activate Virtual Environment:**
    *   Create a Python virtual environment **within this directory**.
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`
    ```

3.  **Install Dependencies:**
    *   Install the required libraries using pip.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set Up Environment Variables:**
    *   **Important:** Copy or rename the `.env.example` file to a new file named exactly `.env` in the same directory.
    *   Edit the `.env` file and fill in **all** the required values:
        *   `GOOGLE_API_KEY`: Your Google API key for Gemini models.
        *   `DB_NAME`: Name for your PostgreSQL database.
        *   `DB_USER`: Your PostgreSQL username.
        *   `DB_PASSWORD`: Your PostgreSQL password.
        *   `DB_HOST`: Database host (e.g., `localhost`).
        *   `DB_PORT`: Database port (e.g., `5432`).
        *   `EBAY_CLIENT_ID`: Your eBay application Client ID.
        *   `EBAY_CLIENT_SECRET`: Your eBay application Client Secret.
        *   `EBAY_USE_SANDBOX`: (Optional) Set to `True` to use the eBay Sandbox environment, otherwise defaults to Production.

5.  **Set Up Database:**
    *   Ensure you have PostgreSQL installed and running.
    *   Create the database specified in your `.env` file.
    *   Run the setup script against your database:
        ```bash
        psql -U <your_db_user> -d <your_db_name> -a -f setup_sales_sim_db.sql
        ```
        (Enter your password when prompted).

6.  **Persona Data Source:**
    *   The simulation now reads persona prompt files (`persona_topic_X_prompt_nmf.txt`) directly from the NMF output directory:
        `../persona_clustering/output/nmf_k20/`
    *   Ensure this directory exists and contains the necessary `.txt` files, likely generated by the `persona_clustering` project.

7.  **Product Index Data Source:**
    *   Similarly, product index files (`persona_topic_X_top_purchases_nmf.csv`) are read from:
        `../persona_clustering/output/nmf_k20/`
    *   These CSV files should contain product details (e.g., `Title`, `Rank`) and must correspond to the personas.
    *   Ensure these files are present in the specified directory before running simulations.

## Running Simulations

### Single Simulation

Use `run_simulation.py` to execute a single conversation for a specific persona.

```bash
# Run with a specific persona (e.g., topic_0)
python run_simulation.py --persona topic_0

# Run with a specific persona and save the full result state to JSON
python run_simulation.py --persona topic_1 --output simulation_results/topic_1_run.json

# Run with the default persona (first one found)
python run_simulation.py
```

### Multiple Runs & Reporting

Use `generate_report.py` to run multiple simulations across various personas, generate sales wisdom using an LLM, calculate metrics, and create a summary report. By default, this script will output the results to `simulation_report.md`.

**Note:** This script can take a significant amount of time to run, depending on the number of personas and runs per persona, as it involves multiple LLM calls and potentially API interactions.

```bash
# Run 1 simulation for all available personas (default), creates simulation_report.md
python generate_report.py

# Run 5 simulations for all available personas
python generate_report.py --runs-per-persona 5

# Run 3 simulations each for specific personas
python generate_report.py --runs-per-persona 3 --personas topic_0 topic_5

# Run 1 simulation for all personas, save to custom report file
python generate_report.py --output-file reports/custom_simulation_report.md

# Run using a different Gemini model for wisdom generation (ensure compatibility)
python generate_report.py --wisdom-model gemini-1.5-pro-latest
```

## Dependencies

*   Python 3.8+
*   See `requirements.txt` for specific Python libraries.
*   PostgreSQL Database

## Configuration

*   Primary configuration is handled via the `.env` file.
*   Simulation parameters (number of runs, personas, etc.) can be controlled via command-line arguments for `generate_report.py`.
*   Constants like maximum conversation turns (`MAX_CONVERSATION_TURNS`) are defined in `core/simulation.py`. 